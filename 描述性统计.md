# 描述性统计

## Introduction

### 研究背景

重要性：精神类疾病到AD 研究AD的必要性


## Method

### XGBoost

  XGBoost（eXtreme Gradient Boosting）是一种强大的机器学习算法，特别适用于回归和分类问题。它是梯度提升决策树（Gradient Boosting Decision Tree）算法的一种实现，通过集成多个决策树模型来进行预测。采用自适应的学习策略，通过迭代地训练决策树并对先前模型的预测误差进行优化，逐步改进整体模型的性能。同时使用梯度提升算法来最小化损失函数，通过计算每个样本的梯度来确定每个决策树的结构和权重。

  XGBoost的特点和性能：
  - 高性能和可扩展性：XGBoost在处理大规模数据集时表现出色，并具有高效的并行计算能力。它可以利用多线程和分布式计算来加速模型的训练和预测过程。
  - 正则化和防止过拟合：XGBoost引入了正则化项和剪枝策略，以减少模型的复杂性并防止过拟合。它提供了各种正则化参数，如学习率、最大深度、子样本比例等，可以根据需要进行调整。
  - 特征重要性评估：XGBoost可以估计每个特征对模型的贡献程度，帮助识别和选择最重要的特征。这种特征重要性评估可以用于特征选择、可视化和模型解释等方面。
  - 缺失值处理：XGBoost能够自动处理缺失值，无需进行数据预处理或填充缺失值。它可以学习如何在决策树中有效地处理缺失值，从而提高模型的鲁棒性。

  XGBoost的工作过程主要包括如下几个步骤：
  - 初始化模型：首先，XGBoost初始化一个简单的基础模型，通常是一个具有常数预测值的决策树。
  - 计算损失函数的梯度和二阶导数：对于训练样本，计算预测值与实际值之间的损失函数的梯度和二阶导数。这些梯度和二阶导数用于度量模型的拟合程度和复杂度。
  - 逐步生成决策树：XGBoost通过迭代地生成决策树来逐步改进模型。每次迭代中，它根据当前模型的预测结果和梯度信息生成一个新的决策树。
  - 计算决策树的分裂质量：在生成决策树的过程中，XGBoost计算每个可能的分裂点的分裂质量，以选择最佳的分裂策略。它使用梯度和二阶导数来评估分裂质量，并考虑正则化项来控制模型的复杂度。
  - 更新模型：通过将新生成的决策树加入到当前模型中，XGBoost更新模型的预测结果。每次迭代时，模型的预测结果会逐步接近实际值，从而提高模型的准确性。
  - 正则化：为了防止过拟合，XGBoost引入了正则化项，如学习率、最大深度、子样本比例等。这些正则化参数可以根据需要进行调整。
  - 重复迭代：重复执行步骤2到步骤6，直到达到预定的迭代次数或满足停止条件（如模型性能不再改善）为止。
  最终，XGBoost将多个决策树的预测结果相加，得到最终的模型预测结果。通过梯度提升的方式，XGBoost能够有效地处理复杂的非线性关系，并具有较强的预测能力和泛化能力。

  XGBoost算法中正则化参数用于控制模型的复杂度和防止过拟合。通过适当调整这些参数，可以平衡模型的拟合能力和泛化能力，提高模型的鲁棒性和稳定性。
  - 学习率（learning rate）：学习率控制每次迭代中新生成决策树的贡献程度。较小的学习率可以使模型更加稳定，但可能需要更多的迭代次数才能达到较好的性能。较大的学习率可以加快模型的收敛速度，但可能导致过拟合。通常需要根据实际情况进行调优。
  - 最大深度（max_depth）：最大深度限制每棵决策树的深度。较小的最大深度可以减少模型的复杂度，防止过拟合，但可能会导致欠拟合。较大的最大深度可以增加模型的拟合能力，但也容易过拟合。需要根据数据集的特点和复杂性来选择合适的最大深度。
  - 子样本比例（subsample）：子样本比例控制每棵决策树在训练过程中使用的样本比例。较小的子样本比例可以减少方差，增强模型的鲁棒性，但可能增加偏差。较大的子样本比例可以减少模型的方差，但也可能导致过拟合。通常使用较小的值（例如0.6-0.8）来提高模型的鲁棒性。
  - 列采样比例（colsample_bytree）：列采样比例控制每棵决策树在训练过程中使用的特征（列）的比例。它可以防止模型对某些特征过于依赖，增加模型的多样性，减少过拟合的风险。通常使用较小的值（例如0.6-0.8）来进行特征采样。
  这些正则化参数可以根据实际问题和数据集的特点进行调优。通常使用交叉验证等技术来选择最佳的正则化参数组合，以在训练集上获得良好的性能，并在测试集上实现较好的泛化能力。正则化参数的合理选择可以提高模型的稳定性、鲁棒性和泛化能力，避免过拟合和欠拟合问题。

  综上所述，XGBoost算法在分类问题、回归问题、排序问题、异常检测、时间序列分析、特征选择和推荐系统等领域具有广泛的应用，成为许多数据科学从业者和研究人员的首选算法之一。

### SVM/SVR

  支持向量机（Support Vector Machine，SVM）是一种经典的监督学习算法，用于解决二分类和多分类问题。它的基本思想是找到一个最优的超平面，能够将不同类别的样本分隔开，并使得边界上的样本点到超平面的距离最大化。在SVM中，样本点被表示为特征向量空间中的点，每个特征都对应于空间中的一个维度。SVM的目标是找到一个超平面，使得正例样本和负例样本能够被最大间隔分开。这个超平面的定义是通过最大化支持向量（位于超平面上的样本点）到超平面的距离来实现的。为了解决非线性可分的问题，SVM引入了核函数（kernel function）。核函数可以将输入数据从原始特征空间映射到高维特征空间，从而使得原本线性不可分的问题在高维空间中变得线性可分。常用的核函数有线性核、多项式核、高斯核（RBF核）等，可以根据数据的特点选择合适的核函数。

  SVM的优点包括：
  - 泛化能力强：SVM通过最大化间隔的方式，使得模型具有较好的泛化能力，能够对新样本进行准确的分类。
  - 对于高维空间处理效果好：SVM在高维空间中进行分类，可以有效地处理特征维度较高的数据。
  - 对于少样本数据有效：SVM通过支持向量的方式，对于少样本数据也能够取得较好的分类效果。
  - 鲁棒性好：SVM对于噪声和异常数据具有一定的鲁棒性，能够有效地处理数据中的噪声问题。

  然而，SVM也有一些限制和注意事项：
  - 计算复杂度高：SVM的训练过程涉及到求解二次规划问题，对于大规模数据集和高维特征空间，计算复杂度较高。
  - 参数调节敏感：SVM中的参数选择对于模型的性能和泛化能力有着重要影响，需要进行合适的参数调节。
  - 对于多类别问题的处理：原始的SVM算法是二分类算法，对于多类别问题需要进行扩展，如使用一对多（One-vs-Rest）或一对一（One-vs-One）策略。

  在实际应用中，SVM被广泛用于文本分类、图像分类、对象检测、人脸识别、生物信息学等领域。通过合适的核函数和参数调节，SVM能够有效地处理复杂的分类问题，并取得较好的性能。

  SVC（Support Vector Classification）是一种基于支持向量机（SVM）的监督学习算法。SVC的目标是找到一个最优的超平面，将不同类别的样本分隔开，并最大化间隔。SVC通过最大化超平面与支持向量之间的间隔来构建分类器，构建出的分类器称为最大间隔分类器。支持向量是离超平面最近的训练样本点，它们决定了超平面的位置和方向。同时，SVC使用核函数来处理非线性可分问题，可以将输入数据从原始特征空间映射到高维特征空间，从而使得原本线性不可分的问题在高维空间中变得线性可分，常用的核函数有线性核、多项式核、高斯核（RBF核）等。此外，SVC引入了软间隔（soft margin）的概念，允许一些样本点位于超平面的错误一侧。通过调节惩罚参数C，可以控制软间隔的宽度。较小的C值会允许更多的分类错误，而较大的C值会强制分类器尽量正确分类所有样本。

  SVC的优点包括
  - 泛化能力强：SVC通过最大化间隔的方式，使得模型具有较好的泛化能力，能够对新样本进行准确的分类。
  - 对于高维空间处理效果好：SVC在高维空间中进行分类，可以有效地处理特征维度较高的数据。
  - 对于少样本数据有效：SVC通过支持向量的方式，对于少样本数据也能够取得较好的分类效果。
  - 鲁棒性好：SVC对于噪声和异常数据具有一定的鲁棒性，能够有效地处理数据中的噪声问题。

  然而，SVC也有一些限制和注意事项：
  - 计算复杂度高：SVC的训练过程涉及到求解二次规划问题，对于大规模数据集和高维特征空间，计算复杂度较高。
  - 参数调节敏感：SVC中的参数选择对于模型的性能和泛化能力有着重要影响，需要进行合适的参数调节。

  原始的SVC算法是二分类算法，但可以通过一对多（One-vs-Rest）或一对一（One-vs-One）策略进行多类别分类。在一对多策略中，针对每个类别训练一个二分类器，将该类别与其他类别作为两个不同的类别进行分类。在一对一策略中，每个类别之间都训练一个二分类器，最后根据投票或决策函数融合来确定最终分类结果。它们的区别在于构建分类器的方式和决策规则。

#### 一对多策略（One-vs-Rest）

  在一对多策略中，对于每个类别，都训练一个二分类器（SVC），每个二分类器将该类别作为正例，将其他所有类别作为负例。例如，对于一个有K个类别的问题，将会构建K个二分类器。第i个二分类器的目标是将第i个类别与其他K-1个类别区分开来。在预测阶段，使用K个二分类器对新样本进行预测。最终的分类结果是具有最高决策函数值的类别。

#### 一对一策略（One-vs-One）

  在一对一策略中，对于K个类别，构建K*(K-1)/2个二分类器（SVC），每个二分类器将两个不同的类别作为正例和负例。例如，对于一个有K个类别的问题，将会构建K*(K-1)/2个二分类器。每个二分类器的目标是将两个特定的类别区分开来。在预测阶段，使用K*(K-1)/2个二分类器对新样本进行预测。使用投票或决策函数融合来确定最终的分类结果。

#### 关于一对多和一对一策略的比较

  一对多策略中构建的分类器数量等于类别数量K，而一对一策略中构建的分类器数量等于K*(K-1)/2。一对多策略中，每个分类器的训练数据都包含正例类别和其他K-1个负例类别的样本，适用于处理类别不平衡的问题；一对一策略中，每个分类器的训练数据只包含两个特定的类别的样本，不需要额外处理类别不平衡。一对多策略中的分类器数量较少，相对计算复杂度较低；一对一策略中的分类器数量较多，相对计算复杂度较高。
一对多策略使用最高决策函数值来确定最终分类结果，而一对一策略使用投票或决策函数融合来确定最终分类结果。

  选择一对多策略还是一对一策略取决于具体的问题和数据集特点。一对多策略适用于类别数量较少或样本不平衡的情况，而一对一策略适用于类别数量较多的情况。此外，一对多策略的计算复杂度较低，一对一策略的计算复杂度较高。因此，在实际应用中需要根据问题的需求和数据集的特点进行选择。
  

## Data Analysis

### 数据集介绍

### 描述性统计

描述图像等
